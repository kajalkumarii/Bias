{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import seaborn as sns\n",
    "from scipy.signal import find_peaks\n",
    "import os\n",
    "import gzip\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add path to the folder\n",
    "\n",
    "The folder has files for each fish by 4 trials.\n",
    "The first two characters of the filename is the fish ID and 5th character is the trial number.\n",
    "A few fishes have only 3 trials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "path = \"/home/kkumari/PhD/fish-data/long-term-free-swim/\"\n",
    "# path = \"C:/PhD/long_term_free_swim\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_collate_data(all_files: List[str], desired_cols: List[str]) -> dict:\n",
    "    fish_data = {}\n",
    "    trial_counter = {}\n",
    "\n",
    "    for file in tqdm(all_files, desc=\"Processing files\"):\n",
    "       \n",
    "        with gzip.open(file, 'rb') as f:\n",
    "            df = pd.read_csv(f, usecols=desired_cols, nrows=1000)\n",
    "        fish_id = os.path.basename(file)[:2]\n",
    "\n",
    "        if fish_id not in fish_data:\n",
    "            fish_data[fish_id] = {'df': [], 'files': []}\n",
    "            trial_counter[fish_id] = 0  # Initialize the trial counter for the fish\n",
    "\n",
    "        trial_counter[fish_id] += 1\n",
    "\n",
    "        if trial_counter[fish_id] <= 3:\n",
    "            fish_data[fish_id]['df'].append(df)\n",
    "            fish_data[fish_id]['files'].append(file)\n",
    "\n",
    "    return fish_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cartesian to spherical coordinates\n",
    "def cart2sph(x,y,z):\n",
    "    azimuth = np.arctan2(y,x)\n",
    "    elevation = np.arctan2(z, np.sqrt(x**2 + y**2))\n",
    "    R = np.sqrt(x**2 + y**2 + z**2)\n",
    "    return(azimuth, elevation, R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_avg_interpolate(df, window_size=11, interpolate_method='pad'):\n",
    "\n",
    "    sdf= df.rolling(window_size,center=True).mean().interpolate(method=interpolate_method)\n",
    "    ddf=sdf.diff().interpolate(method='bfill')\n",
    "    return sdf, ddf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Calculate step sizes and find large steps\n",
    "    df_diff = df.diff(periods=1, axis=0)\n",
    "    steps = np.sqrt(df[\"fishx\"]**2 + df[\"fishy\"]**2 + df[\"fishz\"]**2)\n",
    "    df[\"steps\"] = steps\n",
    "    max_stepsize = 0.02\n",
    "    large_steps = df['steps'] > max_stepsize\n",
    "    count_large_steps = np.count_nonzero(large_steps)\n",
    "    print(f\"Number of large steps: {count_large_steps}\")\n",
    "    w = 10\n",
    "    selected_columns = ['fishz', 'fishy', 'fishx']\n",
    "    large_step_indices = large_steps[large_steps].index.values\n",
    "    for i in range(count_large_steps):\n",
    "        lsi = large_step_indices[i]\n",
    "        df.loc[lsi-w:lsi+w, selected_columns] = np.nan\n",
    "    \n",
    "    # Modify data based on coordinate conversion and thresholds\n",
    "    err = 0.001\n",
    "    df.loc[df['fishz'] < - (0.09 + err), selected_columns] = np.nan\n",
    "    df.loc[df['fishz'] > 0 + err, selected_columns] = np.nan\n",
    "    zoffset = 0.11\n",
    "    azimuth, elevation, R = cart2sph(df['fishx'], df['fishy'], df['fishz'] - zoffset)\n",
    "    err = 0.005\n",
    "    df.loc[R > 0.2 + err, selected_columns] = np.nan\n",
    "    df.loc[R < 0.11 - err, selected_columns] = np.nan\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_angles(df: pd.DataFrame, peaks: np.ndarray) -> Tuple[List[float], List[float]]:\n",
    "    dx = df[\"fishx\"].diff().interpolate(method='bfill')\n",
    "    dy = df[\"fishy\"].diff().interpolate(method='bfill')\n",
    "\n",
    "    angle_wrapped = np.arctan2(dy, dx)\n",
    "    last = 0\n",
    "    angles = []\n",
    "    for phi in angle_wrapped:\n",
    "        while phi < last - np.pi:\n",
    "            phi += 2 * np.pi\n",
    "        while phi > last + np.pi:\n",
    "            phi -= 2 * np.pi\n",
    "        last = phi\n",
    "        angles.append(phi)\n",
    "\n",
    "    angles_at_peaks = [angles[i] for i in peaks]\n",
    "\n",
    "    print(f\"Number of angles: {len(angles)}\")\n",
    "\n",
    "    return angles, angles_at_peaks\n",
    "\n",
    "\n",
    "def calculate_avg_velocity(velocity: pd.Series) -> float:\n",
    "    avg_velocity = velocity.median()\n",
    "    print(f\"Average velocity: {avg_velocity}\")\n",
    "    return avg_velocity\n",
    "\n",
    "\n",
    "def identify_peaks(velocity: pd.Series, fHz: int) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    height = (0.1, 0.5)\n",
    "    frames_btw_2bouts = round(fHz / 10)\n",
    "    bout_width = round(fHz / 100)\n",
    "    prominence = 0.05\n",
    "    peaks, _ = find_peaks(velocity, height=height, distance=frames_btw_2bouts, width=bout_width, prominence=prominence)\n",
    "    print(f\"Number of peaks: {len(peaks)}\")\n",
    "    return peaks, _\n",
    "\n",
    "\n",
    "def calculate_dangles(angles_at_peaks: List[float]) -> np.ndarray:\n",
    "    angles_at_peaks_normalized = np.mod(angles_at_peaks, 2 * np.pi) - np.pi\n",
    "    angles_at_peaks_unwrapped = np.unwrap(angles_at_peaks_normalized)\n",
    "    angles_at_peaks_diff = np.diff(angles_at_peaks_unwrapped)\n",
    "    angles_at_peaks_diff = np.mod(angles_at_peaks_diff + np.pi, 2 * np.pi) - np.pi\n",
    "    dangles = angles_at_peaks_diff\n",
    "    print(f\"Number of dangles: {len(dangles)}\")\n",
    "    return dangles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_additional_variables(fish_data: dict, fHz: int) -> dict:\n",
    "    dangles_dict = {}\n",
    "\n",
    "    for fish_id, data in fish_data.items():\n",
    "        angles_list = []\n",
    "        angles_at_peaks_list = []\n",
    "        avg_velocity_list = []\n",
    "        peak_times_list = []\n",
    "        dangles_list = []\n",
    "        velocity_list = []\n",
    "        peaks_list = []\n",
    "\n",
    "        for df in data['df']:\n",
    "            df[\"realtime\"] = df[\"realtime\"] - df[\"realtime\"].iloc[0]\n",
    "\n",
    "            df = preprocess_data(df)\n",
    "\n",
    "            # Apply moving average and interpolate to find 'dx', 'dy', and 'dz'\n",
    "            df['fishx'], df['dx'] = moving_avg_interpolate(df['fishx'])\n",
    "            df['fishy'], df['dy'] = moving_avg_interpolate(df['fishy'])\n",
    "            df['fishz'], df['dz'] = moving_avg_interpolate(df['fishz'])\n",
    "\n",
    "            velocity = np.sqrt(df[\"dx\"] ** 2 + df[\"dy\"] ** 2 + df[\"dz\"] ** 2) / (1 / fHz)\n",
    "            peaks, _ = identify_peaks(velocity, fHz)\n",
    "            peaks_list.append(peaks)\n",
    "            velocity_list.append(velocity)\n",
    "\n",
    "            angles, angles_at_peaks = calculate_angles(df, peaks)\n",
    "            angles_list.append(angles)\n",
    "            angles_at_peaks_list.append(angles_at_peaks)\n",
    "\n",
    "            avg_velocity = calculate_avg_velocity(velocity)\n",
    "            avg_velocity_list.append(avg_velocity)\n",
    "\n",
    "            peak_times = [df[\"realtime\"].iloc[i] for i in peaks]\n",
    "            peak_times_list.append(peak_times)\n",
    "\n",
    "            dangles = calculate_dangles(angles_at_peaks)\n",
    "            dangles_list.append(dangles)\n",
    "\n",
    "        data['angles'] = angles_list\n",
    "        data['avg_velocity'] = avg_velocity_list\n",
    "        data['angles_at_peaks'] = angles_at_peaks_list\n",
    "        data['peak_times'] = peak_times_list\n",
    "        data['dangles'] = dangles_list\n",
    "        data['velocity'] = velocity_list\n",
    "        data['peaks'] = peaks_list\n",
    "\n",
    "        dangles_dict[fish_id] = dangles_list\n",
    "\n",
    "    return fish_data, dangles_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage\n",
    "all_files = sorted(glob.glob(os.path.join(path, \"*.csv.gz\")))\n",
    "desired_cols = ['fishx', 'fishy', 'fishz', 'realtime']\n",
    "fish_data = load_and_collate_data(all_files, desired_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fish_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fish_data, variables = calculate_additional_variables(fish_data, fHz=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def calculate_additional_variables(fish_data: dict, fHz: int):\n",
    "#     # This function takes a dictionary of fish data and a frequency (fHz) as input.\n",
    "#     # It calculates additional variables based on the fish data and adds them to the dictionary.\n",
    "\n",
    "#     dangles_dict = {}  # Initialize a dictionary to hold the dangles data for each fish\n",
    "\n",
    "#     # Iterate over each fish ID and its corresponding data in the fish_data dictionary\n",
    "#     for fish_id, data in fish_data.items():\n",
    "#         # Iterate over each DataFrame in the 'df' list of the current fish ID\n",
    "#         for df in data['df']:\n",
    "#             # Calculate the time elapsed from the start of recording for each timestamp\n",
    "#             df[\"realtime\"] = df[\"realtime\"] - df[\"realtime\"].iloc[0]\n",
    "\n",
    "#             # Preprocessing code integrated here\n",
    "#             # Calculate step sizes and find large steps\n",
    "#             df = df.diff(periods=1, axis=0)\n",
    "#             steps = np.sqrt(df[\"fishx\"]**2 + df[\"fishy\"]**2 + df[\"fishz\"]**2)\n",
    "#             df[\"steps\"] = steps\n",
    "#             max_stepsize = 0.02  # m : 1.5 body lengths between successive frames\n",
    "#             large_steps = df['steps'] > max_stepsize\n",
    "#             count_large_steps = np.count_nonzero(large_steps)\n",
    "#             w = 10  # frames (represents half the window for deletion)\n",
    "#             selected_columns = ['fishz', 'fishy', 'fishx']\n",
    "#             large_step_indices = large_steps[large_steps].index.values\n",
    "#             for i in range(0, count_large_steps):\n",
    "#                 lsi = large_step_indices[i]\n",
    "#                 df.loc[lsi-w:lsi+w, selected_columns] = np.nan\n",
    "            \n",
    "#             # Modify data based on coordinate conversion and thresholds\n",
    "#             err = 0.001  # accepted error\n",
    "#             df.loc[df['fishz'] < - (0.09 + err), selected_columns] = np.nan\n",
    "#             df.loc[df['fishz'] > 0 + err, selected_columns] = np.nan\n",
    "#             zoffset = 0.11\n",
    "#             azimuth, elevation, R = cart2sph(df['fishx'], df['fishy'], df['fishz'] - zoffset)\n",
    "#             err = 0.005  # accepted error\n",
    "#             df.loc[R > 0.2 + err, selected_columns] = np.nan\n",
    "#             df.loc[R < 0.11 - err, selected_columns] = np.nan\n",
    "            \n",
    "#             # Continue with original code\n",
    "#             # Calculate the differences in position (velocity) in each dimension\n",
    "#             dx = df[\"fishx\"].diff().interpolate(method='bfill')\n",
    "#             dy = df[\"fishy\"].diff().interpolate(method='bfill')\n",
    "#             dz = df[\"fishz\"].diff().interpolate(method='bfill')\n",
    "\n",
    "#             # Add the velocity components to the DataFrame\n",
    "#             df[\"dx\"] = dx\n",
    "#             df[\"dy\"] = dy\n",
    "#             df[\"dz\"] = dz\n",
    "\n",
    "#             # Calculate the angles of motion (wrapped between -pi and pi)\n",
    "#             angle_wrapped = np.arctan2(dy, dx)\n",
    "#             last = 0\n",
    "#             angles = []\n",
    "#             for phi in angle_wrapped:\n",
    "#                 while phi < last - np.pi:\n",
    "#                     phi += 2 * np.pi\n",
    "#                 while phi > last + np.pi:\n",
    "#                     phi -= 2 * np.pi\n",
    "#                 last = phi\n",
    "#                 angles.append(phi)\n",
    "\n",
    "#             # Store the angles in the fish_data dictionary under the current fish ID\n",
    "#             data['angles'] = angles\n",
    "\n",
    "#             # Calculate average velocity for each fish\n",
    "#             dt = 1 / fHz\n",
    "#             velocity = np.sqrt(df[\"dx\"] ** 2 + df[\"dy\"] ** 2 + df[\"dz\"] ** 2) / dt\n",
    "#             avg_velocity = velocity.median()\n",
    "#             data['avg_velocity'] = avg_velocity\n",
    "\n",
    "#             # Find peaks in the velocity signal representing bouts of movement\n",
    "#             height = (0.1, 0.5)\n",
    "#             frames_btw_2bouts = round(fHz / 10)\n",
    "#             bout_width = round(fHz / 100)\n",
    "#             prominence = 0.05\n",
    "#             peaks, _ = find_peaks(velocity, height=height, distance=frames_btw_2bouts, width=bout_width, prominence=prominence)\n",
    "\n",
    "#             # Extract the angles and peak times corresponding to the identified peaks\n",
    "#             angles_at_peaks = [angles[i] for i in peaks]\n",
    "#             data['angles_at_peaks'] = angles_at_peaks\n",
    "#             data['peak_times'] = [df[\"realtime\"].iloc[i] for i in peaks]\n",
    "#             # Normalization of peak angles\n",
    "#             angles_at_peaks_normalized = np.mod(angles_at_peaks, 2 * np.pi) - np.pi\n",
    "#             # Unwrap normalized angles\n",
    "#             angles_at_peaks_unwrapped = np.unwrap(angles_at_peaks_normalized)\n",
    "#              # Calculate differences between consecutive unwrapped angles\n",
    "#             angles_at_peaks_diff = np.diff(angles_at_peaks_unwrapped)\n",
    "#             # Apply modulo arithmetic to ensure range between -π and π\n",
    "#             angles_at_peaks_diff = np.mod(angles_at_peaks_diff + np.pi, 2 * np.pi) - np.pi\n",
    "#             dangles = angles_at_peaks_diff\n",
    "\n",
    "#             dangles_dict[fish_id] = dangles  # Add the dangles data to the dictionary for the current fish ID\n",
    "\n",
    "#     return dangles_dict  # Return the dictionary with dangles data for each fish\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def calculate_turning_angle_properties(dangles):\n",
    "#     # Calculate the number of clockwise and counterclockwise turns\n",
    "    \n",
    "#     counterclockwise_turns = np.sum(dangles > 0)\n",
    "#     clockwise_turns = np.sum(dangles < 0)\n",
    "\n",
    "#     # Calculate probability of clockwise and counterclockwise turns\n",
    "#     probability_counterclockwise_turns = counterclockwise_turns / (counterclockwise_turns + clockwise_turns)\n",
    "#     probability_clockwise_turns = clockwise_turns / (counterclockwise_turns + clockwise_turns)\n",
    "\n",
    "#     # Get sequence of right and left turns as 1 and -1\n",
    "#     turns = np.sign(dangles)\n",
    "    \n",
    "#     def streak_lengths(turns):\n",
    "#         if len(turns) == 0:\n",
    "#             return np.array([])  # return empty array for empty input\n",
    "\n",
    "#         streaks = []\n",
    "#         current_streak = 1  # start with a streak of 1\n",
    "\n",
    "#         for i in range(1, len(turns)):\n",
    "#             if turns[i] == turns[i - 1]:  # if current turn is same as previous\n",
    "#                 current_streak += 1  # increment streak count\n",
    "#             else:  # if current turn is different\n",
    "#                 streaks.append(current_streak)  # add the streak to the list\n",
    "#                 current_streak = 1  # reset streak count\n",
    "\n",
    "#         streaks.append(current_streak)  # add the last streak\n",
    "#         return np.array(streaks)\n",
    "    \n",
    "#     streaks = streak_lengths(turns)\n",
    "    \n",
    "#     # Now let's bundle all this data into a dictionary and return it\n",
    "#     turning_properties = {\n",
    "#         'counterclockwise_turns': counterclockwise_turns,\n",
    "#         'clockwise_turns': clockwise_turns,\n",
    "#         'probability_counterclockwise_turns': probability_counterclockwise_turns,\n",
    "#         'probability_clockwise_turns': probability_clockwise_turns,\n",
    "#         'turns': turns,\n",
    "#         'streaks': streaks\n",
    "#     }\n",
    "\n",
    "#     return turning_properties\n",
    "\n",
    "# dangles_dict = calculate_additional_variables(fish_data, fHz=100)\n",
    "\n",
    "# turning_properties_dict = {}\n",
    "# for fish_id, dangles in dangles_dict.items():\n",
    "#     turning_properties = calculate_turning_angle_properties(dangles)\n",
    "#     turning_properties_dict[fish_id] = turning_properties\n",
    "# turning_properties_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_streak_distribution(turning_properties_dict):\n",
    "#     for fish_id, turning_properties in turning_properties_dict.items():\n",
    "#         streaks = turning_properties['streaks']\n",
    "\n",
    "#         # Plot the distribution of streaks\n",
    "#         plt.hist(streaks, bins='auto')\n",
    "#         plt.xlabel('Streak Length')\n",
    "#         plt.ylabel('Frequency')\n",
    "#         plt.title(f'Fish ID {fish_id} - Streak Distribution')\n",
    "#         plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate_fake_fish_streaks(num_streaks, max_streak_length, avg_streak_number):\n",
    "#     # Generate random turns (-1: left, 1: right)\n",
    "#     turns = np.random.choice([-1, 1], size=num_streaks)\n",
    "\n",
    "#     # Calculate streak lengths\n",
    "#     streak_lengths = []\n",
    "#     current_streak = 0\n",
    "#     for turn in turns:\n",
    "#         if turn == 0:\n",
    "#             current_streak += 1\n",
    "#         else:\n",
    "#             streak_lengths.append(current_streak)\n",
    "#             current_streak = 0\n",
    "#     streak_lengths.append(current_streak)\n",
    "\n",
    "#     # Truncate streak lengths if they exceed the maximum streak length\n",
    "#     streak_lengths = [min(streak, max_streak_length) for streak in streak_lengths]\n",
    "\n",
    "#     # Adjust the length of streak_lengths to match the average streak number\n",
    "#     current_streak_number = len(streak_lengths)\n",
    "#     if current_streak_number < avg_streak_number:\n",
    "#         additional_streaks = avg_streak_number - current_streak_number\n",
    "#         additional_lengths = np.random.randint(1, max_streak_length + 1, size=additional_streaks)\n",
    "#         streak_lengths += list(additional_lengths)\n",
    "\n",
    "#     return streak_lengths\n",
    "\n",
    "\n",
    "# def calculate_average_streak_number(turning_properties_dict):\n",
    "#     total_streak_number = sum([len(props['streaks']) for props in turning_properties_dict.values()])\n",
    "#     total_fish_number = len(turning_properties_dict)\n",
    "#     average_streak_number = total_streak_number // total_fish_number\n",
    "\n",
    "#     return average_streak_number\n",
    "\n",
    "\n",
    "# # Calculate the average streak number from the real fish data\n",
    "# average_streak_number = calculate_average_streak_number(turning_properties_dict)\n",
    "\n",
    "# # Specify the parameters for the fake fish streaks\n",
    "# num_streaks = average_streak_number\n",
    "# max_streak_length = 10\n",
    "\n",
    "# # Generate fake fish streak lengths with the adjusted number of streaks\n",
    "# fake_fish_streaks = generate_fake_fish_streaks(num_streaks, max_streak_length, average_streak_number)\n",
    "\n",
    "# # Plot the streak length distributions with the fake fish streak lengths included\n",
    "# plot_streak_distribution(turning_properties_dict, fake_fish_streaks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Specify the directory path where you want to save the CSV files\n",
    "# output_directory = \"/home/kkumari/PhD/fish-data/output-long-term-free-swim/\"\n",
    "\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "\n",
    "# def save_turning_properties_as_csv(turning_properties_dict, output_directory):\n",
    "#     # Iterate over the dictionary items\n",
    "#     for fish_id, turning_properties in turning_properties_dict.items():\n",
    "#         # Convert the arrays to string representations with proper formatting\n",
    "#         turning_properties_str = {key: np.array2string(value, separator=', ') for key, value in turning_properties.items()}\n",
    "\n",
    "#         # Specify the file path for the current fish ID\n",
    "#         output_path = output_directory + f\"turning_properties_{fish_id}.csv\"\n",
    "\n",
    "#         # Save the dictionary as a CSV file\n",
    "#         df = pd.DataFrame.from_dict(turning_properties_str, orient='index')\n",
    "#         df.to_csv(output_path)\n",
    "\n",
    "# def load_turning_properties_from_csv(file_path):\n",
    "#     # Read the CSV file into a DataFrame\n",
    "#     df = pd.read_csv(file_path, index_col=0)\n",
    "\n",
    "#     # Convert the string representations back to arrays\n",
    "#     turning_properties = {}\n",
    "#     for key in df.index:\n",
    "#         value_str = df.loc[key].values[0]\n",
    "#         value = np.fromstring(value_str[1:-1], sep=', ')\n",
    "#         turning_properties[key] = value\n",
    "\n",
    "#     return turning_properties\n",
    "\n",
    "# # Save the dictionary as CSV files\n",
    "# save_turning_properties_as_csv(turning_properties_dict, output_directory)\n",
    "\n",
    "# # Load the CSV files back into a dictionary with correct array data\n",
    "# loaded_turning_properties_dict = {}\n",
    "# for fish_id in turning_properties_dict:\n",
    "#     file_path = output_directory + f\"turning_properties_{fish_id}.csv\"\n",
    "#     turning_properties = load_turning_properties_from_csv(file_path)\n",
    "#     loaded_turning_properties_dict[fish_id] = turning_properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read all csv files in the directory\n",
    "# all_dict_files = sorted(glob.glob(os.path.join(output_directory, \"*.csv\")))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_streak_distribution(all_dict_files):\n",
    "#     # Iterate over the CSV files\n",
    "#     for file in all_dict_files:\n",
    "#         # Read the CSV file into a DataFrame\n",
    "#         df = pd.read_csv(file)\n",
    "\n",
    "#         # Get the fish ID from the file name\n",
    "#         fish_id = file.split('/')[-1].split('.')[0].split('_')[-1]\n",
    "\n",
    "#         # Check if 'streaks' column exists in the DataFrame\n",
    "#         if 'streaks' not in df.columns:\n",
    "#             print(f\"Streaks column not found in {file}. Skipping...\")\n",
    "#             continue\n",
    "\n",
    "#         # Extract the streaks column\n",
    "#         streaks = df['streaks'].values\n",
    "\n",
    "#         # Plot the distribution of streaks\n",
    "#         plt.hist(streaks, bins='auto')\n",
    "#         plt.xlabel('Streak Length')\n",
    "#         plt.ylabel('Frequency')\n",
    "#         plt.title(f'Fish ID {fish_id} - Streak Distribution')\n",
    "#         plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_streak_distribution(loaded_turning_properties_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  read first file and show the data\n",
    "df = pd.read_csv(all_dict_files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_streak_distribution(all_dict_files):\n",
    "#     # Iterate over the CSV files\n",
    "#     for file in all_dict_files:\n",
    "#         # Read the CSV file into a DataFrame\n",
    "#         df = pd.read_csv(file)\n",
    "\n",
    "#         # Get the fish ID from the file name\n",
    "#         fish_id = file.split('/')[-1].split('.')[0].split('_')[-1]\n",
    "\n",
    "#         # Check if 'streaks' column exists in the DataFrame\n",
    "#         if 'streaks' not in df.columns:\n",
    "#             print(f\"Streaks column not found in {file}. Skipping...\")\n",
    "#             continue\n",
    "\n",
    "#         # Extract the streaks column\n",
    "#         streaks = df['streaks'].values\n",
    "\n",
    "#         # Plot the distribution of streaks\n",
    "#         plt.hist(streaks, bins='auto')\n",
    "#         plt.xlabel('Streak Length')\n",
    "#         plt.ylabel('Frequency')\n",
    "#         plt.title(f'Fish ID {fish_id} - Streak Distribution')\n",
    "#         plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_streak_distribution(all_dict_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def initialize_figure(num_plots: int, figsize: tuple=(12,10)) -> plt.Figure:\n",
    "#     fig, ax = plt.subplots(num_plots, 1, figsize=figsize)\n",
    "#     return fig, ax\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
